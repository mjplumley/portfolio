<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Meredith Plumley</title>
	<atom:link href="https://mjplumley.github.io/portfolio/feed/" rel="self" type="application/rss+xml" />
	<link>http://mjplumley.github.io/portfolio/</link>
	<description>PhD Applied Mathematician </description>
	<lastBuildDate>Fri, 10 Jul 2020 13:30:50 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.2</generator>

<image>
	<url>https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/cropped-LOGOmp-1-32x32.png</url>
	<title>Meredith Plumley</title>
	<link>http://mjplumley.github.io/portfolio/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Rotating fluid dynamics</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/10/rotating-fluid-dynamics/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Fri, 10 Jan 2020 13:37:42 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Fluid Dynamics]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=25</guid>

					<description><![CDATA[My doctoral research focused on rotating, convection driven turbulence and its role in planetary interiors, specifically in the transfer of heat and energy and the&#8230;]]></description>
										<content:encoded><![CDATA[
<p>My doctoral research  focused on rotating, convection driven turbulence and its role in planetary interiors, specifically in the transfer of heat and energy and the formation of structures in the fluid. Rotating turbulence is a key source of energy redistribution within many geophysical and astrophysical flows. These flows are typically dominated by rapid rotation and strong turbulence, which poses a computational challenge for numerical simulation of the governing Naiver&#8211;Stokes equations (NSE).</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-07-at-1.14.12-PM-1024x626.png" alt="" class="wp-image-75" width="497" height="303" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-07-at-1.14.12-PM-1024x626.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-07-at-1.14.12-PM-300x183.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-07-at-1.14.12-PM-768x469.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-07-at-1.14.12-PM.png 1522w" sizes="(max-width: 497px) 100vw, 497px" /><figcaption> <span style="color:#c4edf0" class="has-text-color has-normal-font-size"> Schematic showing the domain of interest from Cheng et al (2015).</span></figcaption></figure></div>



<p>In my dissertation, I developed reduced mathematical models for the study of fluid dynamics in the limit of rapid rotation. The models were analyzed to uncover dominant dynamics and simulated numerically to study the flow morphologies and explore parameter space.</p>



<p>In particular, I explored three different projects in my research: </p>



<ul><li>(i) hydrodynamic fluids focused on the effect of no-slip boundary conditions and comparison to experimental results, with the goal of unifying scaling laws between methodologies; </li><li>(ii) the flow structures as a function of the horizontal domain, with the goal of exploring domains that currently cannot be investigated in laboratory settings; and </li><li>(iii) magnetohydrodynamic fluids, with the goal of understanding the dynamo process occurring within Earth&#8217;s outer iron core. </li></ul>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator"/>
</div></div>
</div></div>



<h3 class="has-text-color has-text-align-center" style="color:#47bec9">Project (i)</h3>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-top" style="grid-template-columns:55% auto"><figure class="wp-block-media-text__media"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.33.08-AM-1024x780.png" alt="" class="wp-image-110" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.33.08-AM-1024x780.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.33.08-AM-300x228.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.33.08-AM-768x585.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.33.08-AM.png 1082w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure><div class="wp-block-media-text__content">
<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.35.14-AM-1024x714.png" alt="" class="wp-image-111" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.35.14-AM-1024x714.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.35.14-AM-300x209.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.35.14-AM-768x536.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-11.35.14-AM.png 1184w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p style="color:#c4edf0" class="has-text-color has-normal-font-size">Visualizations of the heat transfer scaling laws as power laws (left) and as a surface (right).</p>
</div></div>
</div></div>



<div style="height:34px" aria-hidden="true" class="wp-block-spacer"></div>



<p>One major result was the exploration of parameter space with varying boundary conditions.  The wide range of parameter space covered in the study allowed for the development of scaling laws to define how the flow varies with the inputs. Specifically, these scaling laws show the power law dependence of the heat transfer, as measured by the nondimensional Nusselt number <em>Nu</em>, for a wide range of Rayleigh number <em>Ra</em> &#8212; Ekman number <em>E</em> parameter space.</p>



<p>These scaling law results facilitate comparison of experimental and simulation simulations and provide insight into the effect of boundary conditions.</p>



<p>The work and results are detailed in the following papers:</p>



<ul><li>Plumley, Meredith, et al. &#8220;The effects of Ekman pumping on quasi-geostrophic Rayleigh–Bénard convection.&#8221;&nbsp;<em>Journal of Fluid Mechanics</em>&nbsp;803 (2016): 51-71.</li><li>Plumley, Meredith, et al. &#8220;Sensitivity of rapidly rotating Rayleigh-Bénard convection to Ekman pumping.&#8221;&nbsp;<em>Physical Review Fluids</em>&nbsp;2.9 (2017): 094801.</li></ul>



<hr class="wp-block-separator is-style-default"/>



<h3 class="has-text-color has-text-align-center" style="color:#47bec9">Project (ii)</h3>



<div class="wp-block-media-text alignwide is-stacked-on-mobile" style="grid-template-columns:45% auto"><figure class="wp-block-media-text__media"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/combo.png" alt="" class="wp-image-118" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/combo.png 457w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/combo-300x199.png 300w" sizes="(max-width: 457px) 100vw, 457px" /></figure><div class="wp-block-media-text__content">
<p>Departing from the square horizontal domain for the hydrodynamic model, I explored the effect of domain anisotropy on the inverse cascade of energy towards larger scale flows.  For rectangular periodic domains, a Kolmogorov-like flow consisting of a periodic array of alternating unidirectional jets with embedded vortices is observed, together with an underlying weak meandering transverse jet.</p>
</div></div>



<div style="height:29px" aria-hidden="true" class="wp-block-spacer"></div>



<p>This work is detailed in the paper:  </p>



<ul><li>Julien, Keith, Edgar Knobloch, and Meredith Plumley. &#8220;Impact of domain anisotropy on the inverse cascade in geostrophic turbulent convection.&#8221; <em>Journal of Fluid Mechanics</em> 837 (2018).</li></ul>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<hr class="wp-block-separator is-style-default"/>
</div></div>
</div></div>



<h3 class="has-text-color has-text-align-center" style="color:#47bec9">Project (iii)</h3>



<div style="height:34px" aria-hidden="true" class="wp-block-spacer"></div>



<p>For investigations of the geomagnetic field, we simulated a mathematically reduced set of the MHD equations.  I investigated the model using a simplified class of solutions whose horizontal structure is<br>restricted to a certain geometry to reduce the complexity further.  This model contains multiple time scales which add a complication to the numerical simulation. I worked on two methodologies for handling the multiple timescales in the model:</p>



<p></p>



<ul><li> a stiff, common-in-time approach where all timescales are converted to a single time variable and </li><li>a heterogeneous multiscale modeling approach employing fast time averaging on the faster time scale fluxes that feed back onto the slow scales.</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM.png" alt="" class="wp-image-124" width="430" height="134" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM.png 1018w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM-300x94.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM-768x241.png 768w" sizes="(max-width: 430px) 100vw, 430px" /><figcaption> <span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Schematic showing the two methods implemented to compare and validate the handling of the two time scales in the model.</span></figcaption></figure></div>



<p>Each strategy produced consistent results and each illustrated self-similar dynamics as the time averaging window is increased. The model showed that the properties of the convection were significantly altered by the dynamo-generated magnetic field.&nbsp;</p>



<p>These results are detailed in the paper: </p>



<ul><li>Plumley, Meredith, et al. &#8220;Self-consistent single mode investigations of the quasi-geostrophic convection-driven dynamo model.&#8221; <em>Journal of Plasma Physics</em> 84.4 (2018).</li></ul>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Time series sales analysis</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/09/sales-analysis/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Thu, 09 Jan 2020 11:44:16 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=97</guid>

					<description><![CDATA[One of the largest problems retailers face is the prediction of sales. If the exact demand can be known, retailers can avoid the losses from&#8230;]]></description>
										<content:encoded><![CDATA[
<p>One of the largest problems retailers face is the prediction of sales.  If the exact demand can be known, retailers can avoid the losses from being either overstocked or out of stock.  Given the order times that most retailers face, including production and shipping, this means that they must predict the demand at least several months into the future using the data they currently have available.  </p>



<p>This data is taken from an amazon seller, looking to make better forecasting  decisions for product ordering.  </p>



<p></p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy.jpg" alt="" class="wp-image-136" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy.jpg 883w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy-300x119.jpg 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy-768x304.jpg 768w" sizes="(max-width: 883px) 100vw, 883px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> The total sales by state, which correlates with the state populations.</span> </figcaption></figure>



<p>There are several trends evident in the time series of the daily sales.  There are trends on the yearly timescale, including peaks in Q1 and Q3 and a decline during Q2. The massive decline during the COVID-19 crisis in March/April 2020 is evident as a major one-time event.  There is also a larger trend towards fewer sales overall toward the end of 2019 as well as an uptick after May 2020.  </p>



<p>The goal is to be able to capture some of these trends into a prediction of the sales to forecast the order quantities for the seller.  There are several strategies for doing this, two of which I outline below. </p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1024x367.png" alt="" class="wp-image-272" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1024x367.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-300x108.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-768x275.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1536x551.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1920x688.png 1920w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold.png 2042w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> Sales of a specific product over the last 3 years with the rolling averages of 30 days, 60 days and 1 year.</span>  </figcaption></figure>



<hr class="wp-block-separator"/>



<h2 class="has-text-color has-text-align-center" style="color:#47bec9">Formula 1</h2>



<p>This method of prediction is based on the sales during the last 30 and 60 days to capture how the product is currently selling but uses the slope information from previous years to account for historical changes.  This slope is calculated as the rate from the previous year compared to the actual sales and will capture information from yearly repeating but time-isolated increases or decreases in sales, such as for holidays or seasonal variations.  </p>



<p>This slope is multiplied by the averaged past 30 and 60 day sales to get an expected/predicted daily sales value.  The quantity to order can then be calculated as the predicted daily sales rate times the number of days stock you want to have, which is a function of the production, shipping time, and storage fees.  Typically I would take it to be between 30-90 days. </p>



<h3 class="has-text-color has-text-align-center" style="color:#c4edf0"><em>Quantity = predicted average daily sales * days of stock desired</em></h3>



<p>This method relies only on the past sales data to calculate and is less dependent on inputs.</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-color has-text-align-center" style="color:#47bec9">Formula 2</h2>



<p>We can also try to model the daily sales based on the past data and predict the variations of the sales over future days with more precision.  The simplest model to come up with a linear model that uses various features of the data as the variables and fits coefficients to them to measure the influence of those features on the daily sales. This can be expressed as a function like:</p>



<h3 class="has-text-color has-text-align-center" style="color:#c4edf0"><em>Daily Sales = c0 + c1(Mon), + c2(Tue) + &#8230; + c8(Jan) + c9(Feb) + &#8230; + c20(Holiday)</em></h3>



<p>Here the <em>c</em>&#8216;s are the coefficients and the features are represented as <em>Mon</em> for Monday, <em>Tue</em> for Tuesday, and all the other days of the week, the months are represented as <em>Jan</em>, <em>Feb</em>, etc and a few other features are included as well, including if the day is a <em>holiday</em>, the week before a holiday and a counter for annual increase.</p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1024x364.png" alt="" class="wp-image-276" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1024x364.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-300x107.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-768x273.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1536x545.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-2048x727.png 2048w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1920x682.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Sales of a specific product over the last 3 years in orange with the modeled prediction, in blue for the forecast and green for the fit.</span></figcaption></figure>



<p>The model is fit using the Linear Regression from sklearn.  The coefficients it finds are listed in the table below and indicate how each feature impacts the daily sales.  Looking at the day of the week, Monday &#8211; Wedensday account for higher sales.  Looking at the months, April-Jun account for the highest sales.   Holidays lead to less sales on the day and there is an annual increase of 0.3 sales per day. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM.png" alt="" class="wp-image-277" width="374" height="411" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM.png 748w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM-272x300.png 272w" sizes="(max-width: 374px) 100vw, 374px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> List of the values and errors of the coefficients for the fit of the linear model.</span>  </figcaption></figure></div>



<p>This model can also be used to predict the forecasted units by summing over the desired period of time.  This model provides for more daily variation and precision in the forecasting. However, the quality of the model depends highly on the features given to it.  For instance, without the monthly features, the model will not vary on the monthly scale and that seasonal variation will not be included.  More features could be added here to improve the model. </p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Connect4 vs AI (minimax)</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/08/connect4-vs-ai-minimax/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Wed, 08 Jan 2020 07:12:55 +0000</pubDate>
				<category><![CDATA[Data Science]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=215</guid>

					<description><![CDATA[This code allows the user to play the game Connect4 against an AI bot, which chooses its move based on the minimax algorithm. The basic&#8230;]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-buttons alignleft">
<div class="wp-block-button is-style-outline"><a class="wp-block-button__link has-text-color" href="https://mjplumley.github.io/connect4/" style="color:#c4edf0" target="_blank" rel="noreferrer noopener">Play the game</a></div>



<div class="wp-block-button is-style-outline"><a class="wp-block-button__link has-text-color" href="https://github.com/mjplumley/connect4" style="color:#c4edf0" target="_blank" rel="noreferrer noopener">See the code</a></div>
</div>



<p></p>



<p></p>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<p>This code allows the user to play the game Connect4 against an AI bot, which chooses its move based on the minimax algorithm.</p>



<p>The basic idea of the minimax algorithm is that it recurses through the game, trying all possible combinations and keeping track of the winner in each scenario.  The algorithm is programmed to maximize its own success while minimizing its opponents.  Going through every possible combination of play is only possible for extremely small games such as TicTacToe.  Connect4 is too large and the computation would take too long, so I use a technique to shorten the computation: the recursion is stopped after a certain depth.  This depth is how many turns ahead the bot computes before making its decision at each turn.  If no one wins within all the moves at a certain depth, it will pick the options according to the order in which it checks the moves.  So, setting the order in a meaningful way is an important part of the AI strategy as well. </p>



<p>An example of how this works is displayed in the following image.  Assuming the starting board at the top of the image, there are 4 places the AI bot can place their red piece.  None of the possible pieces result in a win or loss for the bot, so all of the scores are 0 at that round.  If the depth was 1, the bot would simply pick whichever play was first in the loop.  </p>



<p>If the depth includes the next turn, then the bot calculates everywhere the yellow pieces could be played by the opponent (us, the human).  Now there are 16  potential boards.  We can see that in 1 of the boards for 3 red choices, yellow will win, which gives the board a score of -1.  For each set of boards on yellow&#8217;s turn, the algorithm with take the minimum score as the score of those moves.  Thus, when it maximizes the potential scores at the end of the algorithm, it will choose the second board option (marked with the green arrow) to avoid the possible boards where yellow wins.  </p>



<p>If we were to include another depth layer, this would result in 64 boards&#8230;which highlights how limiting the depth can be crucial in keeping the computation tractable. </p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1024x577.png" alt="" class="wp-image-222" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1024x577.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-300x169.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-768x432.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1536x865.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM.png 1872w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Starting with the top game board, there are 4 options of where red, the bot, can place their piece (middle row).  The next step is yellow&#8217;s turn and there are 4 places that yellow can play in each board, leading to 16 possible boards (bottom row) after just a depth of 2 for the algorithm.</span> </figcaption></figure>



<p>The code implements 3 different levels of difficulty, which are based on the depth levels.  </p>



<ul><li>Easy: Chooses a depth of 1 or 2 randomly at each turn</li><li>Medium: Maintains a constant depth of 4</li><li>Hard: Begins with a depth of 4 and increases the depth every 4 moves until a maximum depth of 8</li></ul>



<p>The code also implements a weighting of the scoring to encourage the bot to choose the fastest path towards a win.  This is done by using the depth and subtracting a small depth-weighted amount from the score so the fastest win in terms of plays is the highest score. </p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Modeling multiscale equations</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/07/multiscale-equation-modeling/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Tue, 07 Jan 2020 07:36:58 +0000</pubDate>
				<category><![CDATA[Fluid Dynamics]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=231</guid>

					<description><![CDATA[The dynamics of many physical systems are dependent on the interaction of many different scales. Modeling these types of systems leads to difficulties, as the&#8230;]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-buttons">
<div class="wp-block-button is-style-outline"><a class="wp-block-button__link has-text-color" href="https://github.com/mjplumley/SMmultiscale" style="color:#c4edf0" target="_blank" rel="noopener noreferrer">See the code</a></div>
</div>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<p>The dynamics of many physical systems are dependent on the interaction of many different scales.  Modeling these types of systems leads to difficulties, as the range of scales makes direct computation unfeasible.  </p>



<p>There are several methods for handling systems that include a large disparity in scale.  These methods typically involve solving the components separately on different grids with different resolution and coupling the solutions together, using interpolation.  Additionally, there is field called Heterogeneous Multiscale Methods (HMM) which focuses on solving systems of equations that contain the scaling as a parameter.</p>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-image is-style-default"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.12.37-PM.png" alt="" class="wp-image-235" width="339" height="189" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.12.37-PM.png 570w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.12.37-PM-300x167.png 300w" sizes="(max-width: 339px) 100vw, 339px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0">An example of a system with scale separation.  Here the evolution for <em>g</em> is much slower than the evolution of <em>f</em>, and the system is coupled through the functions <em>F</em> and <em>G</em>. </span> </figcaption></figure></div>
</div></div>
</div></div>
</div></div>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container"></div></div>
</div></div>



<p>For investigations of the geomagnetic field, I simulated a mathematically reduced set of the MHD equations. This model contains multiple time scales which added a complication to the numerical simulation.   I investigated the model using a simplified class of solutions whose horizontal structure is restricted to a certain geometry to reduce the complexity further.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM-1024x750.png" alt="" class="wp-image-239" width="405" height="296" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM-1024x750.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM-300x220.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM-768x562.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM-1536x1125.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-14-at-3.19.47-PM.png 1912w" sizes="(max-width: 405px) 100vw, 405px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0">The set of equations with the three scales.</span></figcaption></figure></div>



<p>I worked on two methodologies for handling the multiple timescales in the model:</p>



<ul><li> a stiff, common-in-time approach where all timescales are converted to a single time variable and </li><li>a heterogeneous multiscale modeling approach employing fast time averaging on the faster time scale fluxes that feed back onto the slow scales.</li></ul>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM.png" alt="" class="wp-image-124" width="430" height="134" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM.png 1018w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM-300x94.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Screen-Shot-2020-05-14-at-2.03.31-PM-768x241.png 768w" sizes="(max-width: 430px) 100vw, 430px" /><figcaption> <span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Schematic showing the two methods implemented to compare and validate the handling of the two time scales in the model.</span></figcaption></figure></div>



<p>Each strategy produced consistent results and each illustrated self-similar dynamics as the time averaging window is increased. The model showed that the properties of the convection were significantly altered by the dynamo-generated magnetic field.&nbsp;</p>



<p>These results are detailed in the paper: Plumley, Meredith, et al. &#8220;Self-consistent single mode investigations of the quasi-geostrophic convection-driven dynamo model.&#8221;&nbsp;<em>Journal of Plasma Physics</em>&nbsp;84.4 (2018).</p>



<p></p>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Modeling ash distribution</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/03/modeling-ash-distribution/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Fri, 03 Jan 2020 13:35:36 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Fluid Dynamics]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=23</guid>

					<description><![CDATA[This is a simulation of the ash cloud distribution from the 2010 eruptions of the volcano Eyjafjallajökull in Iceland. I used a tracer model and&#8230;]]></description>
										<content:encoded><![CDATA[
<p>This is a simulation of the ash cloud distribution from the 2010 eruptions of  the volcano Eyjafjallajökull in Iceland.  I used a tracer model and wind data for the first five days after the eruptions to model the path of the ash.  The eruptions on April 14–20 led to a large ash cloud which was blown toward western Europe and was responsible for disrupting plane travel in Europe for 6 days.</p>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/eyjafjallajc3b6kull_volcanic_ash_composite-4.jpg" alt="" data-id="69" data-full-url="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/eyjafjallajc3b6kull_volcanic_ash_composite-4.jpg" data-link="https://mjplumley.github.io/portfolio/2020/01/03/modeling-ash-distribution/eyjafjallajc3b6kull_volcanic_ash_composite-4/#main" class="wp-image-69" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/eyjafjallajc3b6kull_volcanic_ash_composite-4.jpg 726w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/eyjafjallajc3b6kull_volcanic_ash_composite-4-300x203.jpg 300w" sizes="(max-width: 726px) 100vw, 726px" /><figcaption class="blocks-gallery-item__caption"><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> Composite ash cloud on 20 April 2020, looking down from the north pole (wikipedia).</span> </figcaption></figure></li><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Ashplot_recentered-2.jpg" alt="" data-id="67" data-full-url="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Ashplot_recentered-2.jpg" data-link="https://mjplumley.github.io/portfolio/2020/01/03/modeling-ash-distribution/ashplot_recentered-2/#main" class="wp-image-67" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Ashplot_recentered-2.jpg 976w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Ashplot_recentered-2-300x138.jpg 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/Ashplot_recentered-2-768x353.jpg 768w" sizes="(max-width: 976px) 100vw, 976px" /><figcaption class="blocks-gallery-item__caption"><span style="color:#c4edf0" class="has-text-color has-normal-font-size">Simulated ash cloud for April 20.</span></figcaption></figure></li></ul></figure>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p>Comparisons with the composite volcanic ash cloud as of 20 April 2010 show good agreement with the simulated result, including the spread eastward over Russia and minor spread westward towards Greenland as well.</p>



<p></p>



<p>The distribution of the falling ash can be seen in the following figure, which displays the ash cloud as a function of height above sea level each day after the eruption for four days. </p>
</div></div>



<div style="height:31px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-media-text alignwide is-stacked-on-mobile is-vertically-aligned-top" style="grid-template-columns:56% auto"><figure class="wp-block-media-text__media"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/fallplot-1.jpg" alt="" class="wp-image-51" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/fallplot-1.jpg 1008w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/fallplot-1-300x214.jpg 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/fallplot-1-768x549.jpg 768w" sizes="(max-width: 1008px) 100vw, 1008px" /></figure><div class="wp-block-media-text__content">
<p>Using the initial height of the volcanic eruption as a starting point and an estimate for the rate at which ash particles fall, this displays where in longitude the ash will fall. The majority of the ash is shown falling onto Iceland and Western Europe, with a small amount reaching into Russia by the fourth day after the eruption.</p>
</div></div>



<div style="height:28px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container">
<div class="wp-block-group"><div class="wp-block-group__inner-container">
<p>Potential improvements to the model would include factoring in a distribution of ash particle sizes, which would affect the fall rate.  </p>
</div></div>
</div></div>



<div class="wp-block-group"><div class="wp-block-group__inner-container"></div></div>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Recommender Problem: Netflix Data</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/02/recommender-problem-netflix-data/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Thu, 02 Jan 2020 08:54:09 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=145</guid>

					<description><![CDATA[Netflix collects user’s movie ratings in order to better recommend new movies to those users as well as predict how much a different user might&#8230;]]></description>
										<content:encoded><![CDATA[
<p> Netflix collects user’s movie ratings in order to better recommend new movies to those users as well as predict how much a different user might enjoy a certain film. In this project, I explore how, given a large matrix of these rankings, we can minimize the error in our recommendations while keeping the computational cost tractable.  The focus here is on collaborative filtering methods, where &nbsp;the recommendations are based on the ratings of other users.</p>



<p>The data used to construct the matrix <em>A</em> is from <a rel="noreferrer noopener" href="http://movielens.umn.edu" target="_blank">movielens.umn.edu</a>. They collected data during a seven-month period from September 19th, 1997 through April 22nd, 1998. The data consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies [Grouplens]. If a user has not rated a movie, that is left as a zero.</p>



<p></p>



<p>The testing matrix denoted <em>A_test</em> uses a percentage of our data matrix <em>A</em>. Let <em>A_approx</em> be the approximation of the testing matrix <em>A_test</em> produced by the following methods. The error for comparison uses the root mean squared error (RMSE) of <em>A_test </em>&#8211; <em>A_approx</em>.</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center">Nearest Neighbors Method</h2>



<p>The most intuitive method for creating a recommendation is to use the nearest-neighbors method. The idea behind this method is to search through the rest of the users to find others who have similar tastes and use their ratings for the predictions.  This is an example of user-based collaborative filtering. </p>



<p>Formally, this method creates a matrix with values that represent the number of connections between two users, where connections are based off the number of ratings that the users agree on. The connection is calculated using the Guassian kernal. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM.png" alt="" class="wp-image-150" width="235" height="241" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM.png 808w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM-293x300.png 293w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM-768x787.png 768w" sizes="(max-width: 235px) 100vw, 235px" /><figcaption> <span style="color:#c4edf0" class="has-text-color has-normal-font-size"> The Root Mean Squared Error as a function of <em>k</em> neighbors for a test matrix using 35% of the data to test.</span> </figcaption></figure></div>



<p>The computational cost of this method is fairly large. It must calculate the norm difference between all the users over all the non-zero entries of the user whose ratings we are trying to predict. While the matrix is sparse, it is still calculating all the norms before searching for the smallest <em>k</em> entries</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center">Singular Value Decomposition Method</h2>



<p>Another method for investigating this is using a model based system, relying on matrix methods and dimensionality reduction.  The Singular Value Decomposition is a well known factorization method. It decomposes a real or complex rectangular matrix <em>A</em> into the product of three matrices, such that  <em>A = USV</em>.   The first<em> r </em>= rank(<em>A</em>) diagonal entries of <em>S</em>, denoted as sigma, are the ordered singular values of <em>A</em>.  The columns of <em>U</em> and <em>V</em> are, respectively, the left-singular vectors and right-singular vectors of <em>A</em>.</p>



<p>For the Netflix problem, the rows of <em>A </em>represent users and the columns of <em>A</em> represent movies. The SVD of <em>A</em> offers the ability to view hidden features that are not readily represented in the matrix <em>A</em>. For example the hidden features could be genres, actors, or a general preference for movie watching. Thus the matrix <em>U</em> relates users to the feature space and <em>V</em> relates the movies to the feature space. The singular values represented in <em>S</em> scale the features. The higher the singular value, the more important the feature is in relating the users to movies [Ullman].</p>



<p>Due to the size of our problem we would like to reduce the number of computations required. One way to accomplish this is to reduce the dimension of our SVD decomposition to get an approximate matrix to our original. This process is called <strong>Dimensionality Reduction</strong>. One method of dimensionally reduction is called rank reduction, which chooses the largest <em>k &lt; r</em> singular values and their corresponding left and right singular vectors.  We can also note that only the top <em>k</em> features in our feature space are important in determining the ratings.</p>



<p></p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1024x533.png" alt="" class="wp-image-154" width="355" height="184" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1024x533.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-300x156.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-768x400.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1536x799.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM.png 1568w" sizes="(max-width: 355px) 100vw, 355px" /></figure></div>



<p></p>



<p>Let us split the data matrix <em>A</em> into three separate matrices as shown below, where <em>D</em> is the testing matrix.   We then do SVD rank reduction on both B and C such that we choose the top k singular values.  An  approximation of the whole ranking matrix by multiplying the left singular vectors of <em>C</em>, the square root of the singular values of <em>C</em>, the square root of the singular values of <em>B</em> and the right singular vectors of <em>B</em>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-1024x525.png" alt="" class="wp-image-153" width="434" height="222" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-1024x525.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-300x154.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-768x393.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM.png 1296w" sizes="(max-width: 434px) 100vw, 434px" /></figure></div>



<p>The computational cost of this method is fairly small. The most expensive cost is preforming the SVD on the two matrices <em>B </em>and <em>C</em> that act like our training matrices. Once we have the SVD decomposition, forming our testing matrix with a low <em>k</em> value is relatively cheap since our <em>k</em> value is small.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-1024x813.png" alt="" class="wp-image-160" width="395" height="313" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-1024x813.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-300x238.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-768x610.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM.png 1446w" sizes="(max-width: 395px) 100vw, 395px" /><figcaption> <span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Error as a function of the number of singular values included.  The minimum at <em>k</em> = 15 is 1.52 (excluding the case of <em>k</em>=1). </span>  </figcaption></figure></div>



<figure class="wp-block-table aligncenter is-style-stripes"><table><tbody><tr><td><span class="has-inline-color has-very-dark-gray-color">Method</span></td><td><span class="has-inline-color has-very-dark-gray-color">Nearest Neighbors</span></td><td><span class="has-inline-color has-very-dark-gray-color">SVD</span></td></tr><tr><td>Lowest RMS Error</td><td>0.47</td><td>1.52</td></tr></tbody></table></figure>



<div style="height:35px" aria-hidden="true" class="wp-block-spacer"></div>



<p>By RMSE, the Nearest Neighbor method preformed the best.  However, both methods tended to produce many ratings around 3 &#8211; the middle score. Relatively, the data set used for our analysis is small compared to the actual Netflix problem.  While the nearest neighbors method does provide a better RSME, the computational cost makes it intractable for large matrices, and the SVD method provides a cost-effective alternative. </p>



<div style="height:101px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-1024x682.png" alt="" class="wp-image-162" width="536" height="357" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-1024x682.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-768x511.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM.png 1238w" sizes="(max-width: 536px) 100vw, 536px" /></figure></div>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Neural Network XOR</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/01/neural-network-xor/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Wed, 01 Jan 2020 14:45:27 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=175</guid>

					<description><![CDATA[The &#8216;exclusive or &#8216; (XOR) is a simple problem that can be trained using Machine Learning techniques. Here I apply a simple 2 layer neural&#8230;]]></description>
										<content:encoded><![CDATA[
<p>The &#8216;exclusive or &#8216; (XOR) is a simple problem that can be trained using Machine Learning techniques. Here I apply a simple 2 layer neural network written in python and solved using matrix math (no ML packages used) to investigate neural networks on a more fundamental level. </p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile" style="grid-template-columns:22% auto"><figure class="wp-block-media-text__media"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM.png" alt="" class="wp-image-188" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM.png 998w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM-295x300.png 295w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM-768x782.png 768w" sizes="(max-width: 998px) 100vw, 998px" /></figure><div class="wp-block-media-text__content">
<p>The inputs and outputs of XOR are expressed in the image to the left.  There are four input vectors, which are made up of all the combinations of 0 and 1, such as (1,0),(0,1),(0,0), and (1,1).  The output of XOR is 1 if the two inputs are different [(1,0) and (0,1)] and 0 if the inputs are the same [(1,1) and (0,0)].  Thus the training set is 4 vectors.  </p>
</div></div>



<div style="height:27px" aria-hidden="true" class="wp-block-spacer"></div>



<p>If we investigate the solution space [0, 1] x [0, 1], there are many correct solutions to this problem.  The 4 corners of the space are constrained and the system can learn several weights that will give the correct solutions to the training set but with different solutions on the interior of the domain.</p>



<p>When 2 nodes are included in the hidden layer, the result will sometimes give a correct solution (below left) but sometimes not (below right).  Notice that only 2 of the 4 training vectors are correctly satisfied in the solution on the right. The weights are initialized randomly so it might be a function of those starting weights and hysteresis.  </p>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png" alt="" data-id="192" data-full-url="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png" data-link="https://mjplumley.github.io/portfolio/2020/06/02/neural-network-xor/screen-shot-2020-06-03-at-11-07-44-am/#main" class="wp-image-192" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png 850w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM-300x198.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM-768x508.png 768w" sizes="(max-width: 850px) 100vw, 850px" /></figure></li><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM.png" alt="" data-id="191" class="wp-image-191" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM.png 838w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM-768x511.png 768w" sizes="(max-width: 838px) 100vw, 838px" /></figure></li></ul><figcaption class="blocks-gallery-caption"><span style="color:#c4edf0" class="has-inline-color">A correct (left) and incorrect (right) solution.</span></figcaption></figure>



<p>When 4 nodes are included in the hidden layer, the system no longer gets caught between solutions and consistently produces correct solutions. Examples of the many correct solutions are shown below.</p>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM.png" alt="" data-id="190" class="wp-image-190" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM.png 840w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM-768x512.png 768w" sizes="(max-width: 840px) 100vw, 840px" /></figure></li><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1.png" alt="" data-id="195" data-link="https://mjplumley.github.io/portfolio/2020/06/02/neural-network-xor/xor_solutions-1/#main" class="wp-image-195" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1.png 695w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1-300x207.png 300w" sizes="(max-width: 695px) 100vw, 695px" /></figure></li></ul><figcaption class="blocks-gallery-caption"><span style="color:#c4edf0" class="has-inline-color">All correct solutions.</span></figcaption></figure>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center"><strong>Code</strong></h2>



<h4><strong>Main File</strong></h4>



<pre class="wp-block-code"><code>import numpy as np
import random
import matplotlib.pyplot as plt
# NEED TO IMPORT THE NEURALNETWORK CLASS FILE

training_data = &#91; &#91;1,1,0], &#91;0,0,0],&#91;1,0,1],&#91;0,1,1]  ]
brain = NeuralNetwork(2,4,1)
# TRAINING IT FOR THE XOR 
for i in range(50000):
    r = np.floor(4*(random.random()))
    data = training_data&#91;int(r)] 
    brain.train(data&#91;:2],data&#91;2])
 
# PLOT THE RESULTS OVER THE &#91;0,1]x&#91;0,1] DOMAIN
x = y = np.arange(0, 1.0, 0.01)
X, Y = np.meshgrid(x, y)
Z = np.zeros(&#91;100,100])
for ptx in x:
    for pty in y:
        Z&#91;int(100*ptx + .01), int(100*pty + .01)] = brain.feed_forward(&#91;ptx,pty])
plt.contourf(X, Y, Z, 20, cmap='RdGy')
plt.colorbar();</code></pre>



<h4><strong>NeuralNetwork Class File</strong></h4>



<pre class="wp-block-code"><code>import numpy as np
import random

def sigmoid(x):
    return 1/(1 + np.exp(-x))


class NeuralNetwork:
    
    def __init__(self, numI, numH, numO, lr = 0.1):
        self.input_nodes =  numI
        self.hidden_nodes = numH
        self.output_nodes = numO
        
        self.weights_ih = np.random.rand(numH,numI)
        self.weights_ho = np.random.rand(numO,numH)
        
        self.bias_h = np.random.rand(numH)
        self.bias_o = np.random.rand(numO)
        self.lr = lr

        
    def feed_forward(self, inputs):
        hidden = np.dot(self.weights_ih,inputs)
        hidden = hidden + self.bias_h
        # ACTIVATION
        hidden = sigmoid(hidden) 
        
        output = np.dot(self.weights_ho, hidden)
        guess = sigmoid(output + self.bias_o)
        return guess
    
    
    def train(self, inputs, targets):
        # Feed forward in here to have the hidden 
        hidden = np.dot(self.weights_ih,inputs)
        hidden = hidden + self.bias_h
        # ACTIVATION
        hidden = sigmoid(hidden) 
        
        outputs = np.dot(self.weights_ho, hidden)
        outputs = sigmoid(outputs + self.bias_o)
        
        output_errors = targets - outputs
        gradient_output = outputs*(1-outputs)
        weights_ho_change = self.lr*np.outer(gradient_output* output_errors, hidden.T)
        self.weights_ho += weights_ho_change
        self.bias_o += self.lr*gradient_output*output_errors
        
        hidden_errors = np.dot(self.weights_ho.T, output_errors)
        gradient_hidden = hidden*(1-hidden)
        weights_ih_change = self.lr*np.outer(gradient_hidden* hidden_errors, inputs)
        self.weights_ih += weights_ih_change
        self.bias_h += self.lr*gradient_hidden*hidden_errors
        </code></pre>



<p></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
