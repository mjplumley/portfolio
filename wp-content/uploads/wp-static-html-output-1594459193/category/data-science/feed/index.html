<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Data Science &#8211; Meredith Plumley</title>
	<atom:link href="https://mjplumley.github.io/portfolio/category/data-science/feed/" rel="self" type="application/rss+xml" />
	<link>http://mjplumley.github.io/portfolio/</link>
	<description>PhD Applied Mathematician </description>
	<lastBuildDate>Fri, 10 Jul 2020 13:30:50 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.4.2</generator>

<image>
	<url>https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/cropped-LOGOmp-1-32x32.png</url>
	<title>Data Science &#8211; Meredith Plumley</title>
	<link>http://mjplumley.github.io/portfolio/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Time series sales analysis</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/09/sales-analysis/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Thu, 09 Jan 2020 11:44:16 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=97</guid>

					<description><![CDATA[One of the largest problems retailers face is the prediction of sales. If the exact demand can be known, retailers can avoid the losses from&#8230;]]></description>
										<content:encoded><![CDATA[
<p>One of the largest problems retailers face is the prediction of sales.  If the exact demand can be known, retailers can avoid the losses from being either overstocked or out of stock.  Given the order times that most retailers face, including production and shipping, this means that they must predict the demand at least several months into the future using the data they currently have available.  </p>



<p>This data is taken from an amazon seller, looking to make better forecasting  decisions for product ordering.  </p>



<p></p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy.jpg" alt="" class="wp-image-136" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy.jpg 883w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy-300x119.jpg 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/05/sales_by_state19_20-copy-768x304.jpg 768w" sizes="(max-width: 883px) 100vw, 883px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> The total sales by state, which correlates with the state populations.</span> </figcaption></figure>



<p>There are several trends evident in the time series of the daily sales.  There are trends on the yearly timescale, including peaks in Q1 and Q3 and a decline during Q2. The massive decline during the COVID-19 crisis in March/April 2020 is evident as a major one-time event.  There is also a larger trend towards fewer sales overall toward the end of 2019 as well as an uptick after May 2020.  </p>



<p>The goal is to be able to capture some of these trends into a prediction of the sales to forecast the order quantities for the seller.  There are several strategies for doing this, two of which I outline below. </p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1024x367.png" alt="" class="wp-image-272" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1024x367.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-300x108.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-768x275.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1536x551.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold-1920x688.png 1920w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/sales_daily_tanold.png 2042w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> Sales of a specific product over the last 3 years with the rolling averages of 30 days, 60 days and 1 year.</span>  </figcaption></figure>



<hr class="wp-block-separator"/>



<h2 class="has-text-color has-text-align-center" style="color:#47bec9">Formula 1</h2>



<p>This method of prediction is based on the sales during the last 30 and 60 days to capture how the product is currently selling but uses the slope information from previous years to account for historical changes.  This slope is calculated as the rate from the previous year compared to the actual sales and will capture information from yearly repeating but time-isolated increases or decreases in sales, such as for holidays or seasonal variations.  </p>



<p>This slope is multiplied by the averaged past 30 and 60 day sales to get an expected/predicted daily sales value.  The quantity to order can then be calculated as the predicted daily sales rate times the number of days stock you want to have, which is a function of the production, shipping time, and storage fees.  Typically I would take it to be between 30-90 days. </p>



<h3 class="has-text-color has-text-align-center" style="color:#c4edf0"><em>Quantity = predicted average daily sales * days of stock desired</em></h3>



<p>This method relies only on the past sales data to calculate and is less dependent on inputs.</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-color has-text-align-center" style="color:#47bec9">Formula 2</h2>



<p>We can also try to model the daily sales based on the past data and predict the variations of the sales over future days with more precision.  The simplest model to come up with a linear model that uses various features of the data as the variables and fits coefficients to them to measure the influence of those features on the daily sales. This can be expressed as a function like:</p>



<h3 class="has-text-color has-text-align-center" style="color:#c4edf0"><em>Daily Sales = c0 + c1(Mon), + c2(Tue) + &#8230; + c8(Jan) + c9(Feb) + &#8230; + c20(Holiday)</em></h3>



<p>Here the <em>c</em>&#8216;s are the coefficients and the features are represented as <em>Mon</em> for Monday, <em>Tue</em> for Tuesday, and all the other days of the week, the months are represented as <em>Jan</em>, <em>Feb</em>, etc and a few other features are included as well, including if the day is a <em>holiday</em>, the week before a holiday and a counter for annual increase.</p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1024x364.png" alt="" class="wp-image-276" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1024x364.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-300x107.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-768x273.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1536x545.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-2048x727.png 2048w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.51-PM-1920x682.png 1920w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Sales of a specific product over the last 3 years in orange with the modeled prediction, in blue for the forecast and green for the fit.</span></figcaption></figure>



<p>The model is fit using the Linear Regression from sklearn.  The coefficients it finds are listed in the table below and indicate how each feature impacts the daily sales.  Looking at the day of the week, Monday &#8211; Wedensday account for higher sales.  Looking at the months, April-Jun account for the highest sales.   Holidays lead to less sales on the day and there is an annual increase of 0.3 sales per day. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM.png" alt="" class="wp-image-277" width="374" height="411" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM.png 748w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/07/Screen-Shot-2020-07-10-at-2.51.59-PM-272x300.png 272w" sizes="(max-width: 374px) 100vw, 374px" /><figcaption><span style="color:#c4edf0" class="has-text-color has-normal-font-size"> List of the values and errors of the coefficients for the fit of the linear model.</span>  </figcaption></figure></div>



<p>This model can also be used to predict the forecasted units by summing over the desired period of time.  This model provides for more daily variation and precision in the forecasting. However, the quality of the model depends highly on the features given to it.  For instance, without the monthly features, the model will not vary on the monthly scale and that seasonal variation will not be included.  More features could be added here to improve the model. </p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Connect4 vs AI (minimax)</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/08/connect4-vs-ai-minimax/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Wed, 08 Jan 2020 07:12:55 +0000</pubDate>
				<category><![CDATA[Data Science]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=215</guid>

					<description><![CDATA[This code allows the user to play the game Connect4 against an AI bot, which chooses its move based on the minimax algorithm. The basic&#8230;]]></description>
										<content:encoded><![CDATA[
<div class="wp-block-buttons alignleft">
<div class="wp-block-button is-style-outline"><a class="wp-block-button__link has-text-color" href="https://mjplumley.github.io/connect4/" style="color:#c4edf0" target="_blank" rel="noreferrer noopener">Play the game</a></div>



<div class="wp-block-button is-style-outline"><a class="wp-block-button__link has-text-color" href="https://github.com/mjplumley/connect4" style="color:#c4edf0" target="_blank" rel="noreferrer noopener">See the code</a></div>
</div>



<p></p>



<p></p>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<p>This code allows the user to play the game Connect4 against an AI bot, which chooses its move based on the minimax algorithm.</p>



<p>The basic idea of the minimax algorithm is that it recurses through the game, trying all possible combinations and keeping track of the winner in each scenario.  The algorithm is programmed to maximize its own success while minimizing its opponents.  Going through every possible combination of play is only possible for extremely small games such as TicTacToe.  Connect4 is too large and the computation would take too long, so I use a technique to shorten the computation: the recursion is stopped after a certain depth.  This depth is how many turns ahead the bot computes before making its decision at each turn.  If no one wins within all the moves at a certain depth, it will pick the options according to the order in which it checks the moves.  So, setting the order in a meaningful way is an important part of the AI strategy as well. </p>



<p>An example of how this works is displayed in the following image.  Assuming the starting board at the top of the image, there are 4 places the AI bot can place their red piece.  None of the possible pieces result in a win or loss for the bot, so all of the scores are 0 at that round.  If the depth was 1, the bot would simply pick whichever play was first in the loop.  </p>



<p>If the depth includes the next turn, then the bot calculates everywhere the yellow pieces could be played by the opponent (us, the human).  Now there are 16  potential boards.  We can see that in 1 of the boards for 3 red choices, yellow will win, which gives the board a score of -1.  For each set of boards on yellow&#8217;s turn, the algorithm with take the minimum score as the score of those moves.  Thus, when it maximizes the potential scores at the end of the algorithm, it will choose the second board option (marked with the green arrow) to avoid the possible boards where yellow wins.  </p>



<p>If we were to include another depth layer, this would result in 64 boards&#8230;which highlights how limiting the depth can be crucial in keeping the computation tractable. </p>



<figure class="wp-block-image size-large"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1024x577.png" alt="" class="wp-image-222" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1024x577.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-300x169.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-768x432.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM-1536x865.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-12-at-9.12.49-AM.png 1872w" sizes="(max-width: 1024px) 100vw, 1024px" /><figcaption><span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Starting with the top game board, there are 4 options of where red, the bot, can place their piece (middle row).  The next step is yellow&#8217;s turn and there are 4 places that yellow can play in each board, leading to 16 possible boards (bottom row) after just a depth of 2 for the algorithm.</span> </figcaption></figure>



<p>The code implements 3 different levels of difficulty, which are based on the depth levels.  </p>



<ul><li>Easy: Chooses a depth of 1 or 2 randomly at each turn</li><li>Medium: Maintains a constant depth of 4</li><li>Hard: Begins with a depth of 4 and increases the depth every 4 moves until a maximum depth of 8</li></ul>



<p>The code also implements a weighting of the scoring to encourage the bot to choose the fastest path towards a win.  This is done by using the depth and subtracting a small depth-weighted amount from the score so the fastest win in terms of plays is the highest score. </p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Recommender Problem: Netflix Data</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/02/recommender-problem-netflix-data/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Thu, 02 Jan 2020 08:54:09 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=145</guid>

					<description><![CDATA[Netflix collects user’s movie ratings in order to better recommend new movies to those users as well as predict how much a different user might&#8230;]]></description>
										<content:encoded><![CDATA[
<p> Netflix collects user’s movie ratings in order to better recommend new movies to those users as well as predict how much a different user might enjoy a certain film. In this project, I explore how, given a large matrix of these rankings, we can minimize the error in our recommendations while keeping the computational cost tractable.  The focus here is on collaborative filtering methods, where &nbsp;the recommendations are based on the ratings of other users.</p>



<p>The data used to construct the matrix <em>A</em> is from <a rel="noreferrer noopener" href="http://movielens.umn.edu" target="_blank">movielens.umn.edu</a>. They collected data during a seven-month period from September 19th, 1997 through April 22nd, 1998. The data consists of 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies [Grouplens]. If a user has not rated a movie, that is left as a zero.</p>



<p></p>



<p>The testing matrix denoted <em>A_test</em> uses a percentage of our data matrix <em>A</em>. Let <em>A_approx</em> be the approximation of the testing matrix <em>A_test</em> produced by the following methods. The error for comparison uses the root mean squared error (RMSE) of <em>A_test </em>&#8211; <em>A_approx</em>.</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center">Nearest Neighbors Method</h2>



<p>The most intuitive method for creating a recommendation is to use the nearest-neighbors method. The idea behind this method is to search through the rest of the users to find others who have similar tastes and use their ratings for the predictions.  This is an example of user-based collaborative filtering. </p>



<p>Formally, this method creates a matrix with values that represent the number of connections between two users, where connections are based off the number of ratings that the users agree on. The connection is calculated using the Guassian kernal. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM.png" alt="" class="wp-image-150" width="235" height="241" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM.png 808w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM-293x300.png 293w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.07.10-AM-768x787.png 768w" sizes="(max-width: 235px) 100vw, 235px" /><figcaption> <span style="color:#c4edf0" class="has-text-color has-normal-font-size"> The Root Mean Squared Error as a function of <em>k</em> neighbors for a test matrix using 35% of the data to test.</span> </figcaption></figure></div>



<p>The computational cost of this method is fairly large. It must calculate the norm difference between all the users over all the non-zero entries of the user whose ratings we are trying to predict. While the matrix is sparse, it is still calculating all the norms before searching for the smallest <em>k</em> entries</p>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center">Singular Value Decomposition Method</h2>



<p>Another method for investigating this is using a model based system, relying on matrix methods and dimensionality reduction.  The Singular Value Decomposition is a well known factorization method. It decomposes a real or complex rectangular matrix <em>A</em> into the product of three matrices, such that  <em>A = USV</em>.   The first<em> r </em>= rank(<em>A</em>) diagonal entries of <em>S</em>, denoted as sigma, are the ordered singular values of <em>A</em>.  The columns of <em>U</em> and <em>V</em> are, respectively, the left-singular vectors and right-singular vectors of <em>A</em>.</p>



<p>For the Netflix problem, the rows of <em>A </em>represent users and the columns of <em>A</em> represent movies. The SVD of <em>A</em> offers the ability to view hidden features that are not readily represented in the matrix <em>A</em>. For example the hidden features could be genres, actors, or a general preference for movie watching. Thus the matrix <em>U</em> relates users to the feature space and <em>V</em> relates the movies to the feature space. The singular values represented in <em>S</em> scale the features. The higher the singular value, the more important the feature is in relating the users to movies [Ullman].</p>



<p>Due to the size of our problem we would like to reduce the number of computations required. One way to accomplish this is to reduce the dimension of our SVD decomposition to get an approximate matrix to our original. This process is called <strong>Dimensionality Reduction</strong>. One method of dimensionally reduction is called rank reduction, which chooses the largest <em>k &lt; r</em> singular values and their corresponding left and right singular vectors.  We can also note that only the top <em>k</em> features in our feature space are important in determining the ratings.</p>



<p></p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1024x533.png" alt="" class="wp-image-154" width="355" height="184" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1024x533.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-300x156.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-768x400.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM-1536x799.png 1536w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.55.51-AM.png 1568w" sizes="(max-width: 355px) 100vw, 355px" /></figure></div>



<p></p>



<p>Let us split the data matrix <em>A</em> into three separate matrices as shown below, where <em>D</em> is the testing matrix.   We then do SVD rank reduction on both B and C such that we choose the top k singular values.  An  approximation of the whole ranking matrix by multiplying the left singular vectors of <em>C</em>, the square root of the singular values of <em>C</em>, the square root of the singular values of <em>B</em> and the right singular vectors of <em>B</em>. </p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-1024x525.png" alt="" class="wp-image-153" width="434" height="222" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-1024x525.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-300x154.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM-768x393.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-10.56.04-AM.png 1296w" sizes="(max-width: 434px) 100vw, 434px" /></figure></div>



<p>The computational cost of this method is fairly small. The most expensive cost is preforming the SVD on the two matrices <em>B </em>and <em>C</em> that act like our training matrices. Once we have the SVD decomposition, forming our testing matrix with a low <em>k</em> value is relatively cheap since our <em>k</em> value is small.</p>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-1024x813.png" alt="" class="wp-image-160" width="395" height="313" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-1024x813.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-300x238.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM-768x610.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-11.57.23-AM.png 1446w" sizes="(max-width: 395px) 100vw, 395px" /><figcaption> <span class="has-text-color has-normal-font-size" style="color:#c4edf0"> Error as a function of the number of singular values included.  The minimum at <em>k</em> = 15 is 1.52 (excluding the case of <em>k</em>=1). </span>  </figcaption></figure></div>



<figure class="wp-block-table aligncenter is-style-stripes"><table><tbody><tr><td><span class="has-inline-color has-very-dark-gray-color">Method</span></td><td><span class="has-inline-color has-very-dark-gray-color">Nearest Neighbors</span></td><td><span class="has-inline-color has-very-dark-gray-color">SVD</span></td></tr><tr><td>Lowest RMS Error</td><td>0.47</td><td>1.52</td></tr></tbody></table></figure>



<div style="height:35px" aria-hidden="true" class="wp-block-spacer"></div>



<p>By RMSE, the Nearest Neighbor method preformed the best.  However, both methods tended to produce many ratings around 3 &#8211; the middle score. Relatively, the data set used for our analysis is small compared to the actual Netflix problem.  While the nearest neighbors method does provide a better RSME, the computational cost makes it intractable for large matrices, and the SVD method provides a cost-effective alternative. </p>



<div style="height:101px" aria-hidden="true" class="wp-block-spacer"></div>



<div class="wp-block-image"><figure class="aligncenter size-large is-resized"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-1024x682.png" alt="" class="wp-image-162" width="536" height="357" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-1024x682.png 1024w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM-768x511.png 768w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-02-at-12.00.32-PM.png 1238w" sizes="(max-width: 536px) 100vw, 536px" /></figure></div>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Neural Network XOR</title>
		<link>https://mjplumley.github.io/portfolio/2020/01/01/neural-network-xor/</link>
		
		<dc:creator><![CDATA[mjplumley]]></dc:creator>
		<pubDate>Wed, 01 Jan 2020 14:45:27 +0000</pubDate>
				<category><![CDATA[All]]></category>
		<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">https://mjplumley.github.io/portfolio/?p=175</guid>

					<description><![CDATA[The &#8216;exclusive or &#8216; (XOR) is a simple problem that can be trained using Machine Learning techniques. Here I apply a simple 2 layer neural&#8230;]]></description>
										<content:encoded><![CDATA[
<p>The &#8216;exclusive or &#8216; (XOR) is a simple problem that can be trained using Machine Learning techniques. Here I apply a simple 2 layer neural network written in python and solved using matrix math (no ML packages used) to investigate neural networks on a more fundamental level. </p>



<div class="wp-block-media-text alignwide is-stacked-on-mobile" style="grid-template-columns:22% auto"><figure class="wp-block-media-text__media"><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM.png" alt="" class="wp-image-188" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM.png 998w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM-295x300.png 295w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-10.51.48-AM-768x782.png 768w" sizes="(max-width: 998px) 100vw, 998px" /></figure><div class="wp-block-media-text__content">
<p>The inputs and outputs of XOR are expressed in the image to the left.  There are four input vectors, which are made up of all the combinations of 0 and 1, such as (1,0),(0,1),(0,0), and (1,1).  The output of XOR is 1 if the two inputs are different [(1,0) and (0,1)] and 0 if the inputs are the same [(1,1) and (0,0)].  Thus the training set is 4 vectors.  </p>
</div></div>



<div style="height:27px" aria-hidden="true" class="wp-block-spacer"></div>



<p>If we investigate the solution space [0, 1] x [0, 1], there are many correct solutions to this problem.  The 4 corners of the space are constrained and the system can learn several weights that will give the correct solutions to the training set but with different solutions on the interior of the domain.</p>



<p>When 2 nodes are included in the hidden layer, the result will sometimes give a correct solution (below left) but sometimes not (below right).  Notice that only 2 of the 4 training vectors are correctly satisfied in the solution on the right. The weights are initialized randomly so it might be a function of those starting weights and hysteresis.  </p>



<figure class="wp-block-gallery columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png" alt="" data-id="192" data-full-url="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png" data-link="https://mjplumley.github.io/portfolio/2020/06/02/neural-network-xor/screen-shot-2020-06-03-at-11-07-44-am/#main" class="wp-image-192" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM.png 850w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM-300x198.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.07.44-AM-768x508.png 768w" sizes="(max-width: 850px) 100vw, 850px" /></figure></li><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM.png" alt="" data-id="191" class="wp-image-191" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM.png 838w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.09.17-AM-768x511.png 768w" sizes="(max-width: 838px) 100vw, 838px" /></figure></li></ul><figcaption class="blocks-gallery-caption"><span style="color:#c4edf0" class="has-inline-color">A correct (left) and incorrect (right) solution.</span></figcaption></figure>



<p>When 4 nodes are included in the hidden layer, the system no longer gets caught between solutions and consistently produces correct solutions. Examples of the many correct solutions are shown below.</p>



<figure class="wp-block-gallery aligncenter columns-2 is-cropped"><ul class="blocks-gallery-grid"><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM.png" alt="" data-id="190" class="wp-image-190" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM.png 840w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM-300x200.png 300w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/Screen-Shot-2020-06-03-at-11.02.30-AM-768x512.png 768w" sizes="(max-width: 840px) 100vw, 840px" /></figure></li><li class="blocks-gallery-item"><figure><img src="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1.png" alt="" data-id="195" data-link="https://mjplumley.github.io/portfolio/2020/06/02/neural-network-xor/xor_solutions-1/#main" class="wp-image-195" srcset="https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1.png 695w, https://mjplumley.github.io/portfolio/wp-content/uploads/2020/06/XOR_solutions-1-300x207.png 300w" sizes="(max-width: 695px) 100vw, 695px" /></figure></li></ul><figcaption class="blocks-gallery-caption"><span style="color:#c4edf0" class="has-inline-color">All correct solutions.</span></figcaption></figure>



<hr class="wp-block-separator"/>



<h2 class="has-text-align-center"><strong>Code</strong></h2>



<h4><strong>Main File</strong></h4>



<pre class="wp-block-code"><code>import numpy as np
import random
import matplotlib.pyplot as plt
# NEED TO IMPORT THE NEURALNETWORK CLASS FILE

training_data = &#91; &#91;1,1,0], &#91;0,0,0],&#91;1,0,1],&#91;0,1,1]  ]
brain = NeuralNetwork(2,4,1)
# TRAINING IT FOR THE XOR 
for i in range(50000):
    r = np.floor(4*(random.random()))
    data = training_data&#91;int(r)] 
    brain.train(data&#91;:2],data&#91;2])
 
# PLOT THE RESULTS OVER THE &#91;0,1]x&#91;0,1] DOMAIN
x = y = np.arange(0, 1.0, 0.01)
X, Y = np.meshgrid(x, y)
Z = np.zeros(&#91;100,100])
for ptx in x:
    for pty in y:
        Z&#91;int(100*ptx + .01), int(100*pty + .01)] = brain.feed_forward(&#91;ptx,pty])
plt.contourf(X, Y, Z, 20, cmap='RdGy')
plt.colorbar();</code></pre>



<h4><strong>NeuralNetwork Class File</strong></h4>



<pre class="wp-block-code"><code>import numpy as np
import random

def sigmoid(x):
    return 1/(1 + np.exp(-x))


class NeuralNetwork:
    
    def __init__(self, numI, numH, numO, lr = 0.1):
        self.input_nodes =  numI
        self.hidden_nodes = numH
        self.output_nodes = numO
        
        self.weights_ih = np.random.rand(numH,numI)
        self.weights_ho = np.random.rand(numO,numH)
        
        self.bias_h = np.random.rand(numH)
        self.bias_o = np.random.rand(numO)
        self.lr = lr

        
    def feed_forward(self, inputs):
        hidden = np.dot(self.weights_ih,inputs)
        hidden = hidden + self.bias_h
        # ACTIVATION
        hidden = sigmoid(hidden) 
        
        output = np.dot(self.weights_ho, hidden)
        guess = sigmoid(output + self.bias_o)
        return guess
    
    
    def train(self, inputs, targets):
        # Feed forward in here to have the hidden 
        hidden = np.dot(self.weights_ih,inputs)
        hidden = hidden + self.bias_h
        # ACTIVATION
        hidden = sigmoid(hidden) 
        
        outputs = np.dot(self.weights_ho, hidden)
        outputs = sigmoid(outputs + self.bias_o)
        
        output_errors = targets - outputs
        gradient_output = outputs*(1-outputs)
        weights_ho_change = self.lr*np.outer(gradient_output* output_errors, hidden.T)
        self.weights_ho += weights_ho_change
        self.bias_o += self.lr*gradient_output*output_errors
        
        hidden_errors = np.dot(self.weights_ho.T, output_errors)
        gradient_hidden = hidden*(1-hidden)
        weights_ih_change = self.lr*np.outer(gradient_hidden* hidden_errors, inputs)
        self.weights_ih += weights_ih_change
        self.bias_h += self.lr*gradient_hidden*hidden_errors
        </code></pre>



<p></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
